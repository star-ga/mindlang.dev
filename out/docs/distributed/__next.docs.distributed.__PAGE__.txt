1:"$Sreact.fragment"
2:I[64798,["/_next/static/chunks/79131a45f9b8ac11.js","/_next/static/chunks/27be2e6eef9e968f.js"],"DocsSidebar"]
3:I[41146,["/_next/static/chunks/79131a45f9b8ac11.js","/_next/static/chunks/27be2e6eef9e968f.js"],"DocsHeader"]
4:I[22016,["/_next/static/chunks/79131a45f9b8ac11.js","/_next/static/chunks/27be2e6eef9e968f.js"],""]
5:I[61939,["/_next/static/chunks/79131a45f9b8ac11.js","/_next/static/chunks/27be2e6eef9e968f.js"],"CodeBlock"]
1b:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"OutletBoundary"]
1c:"$Sreact.suspense"
0:{"buildId":"jRWSsDu5NohEzxGUsddsH","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"container !pt-12 !pb-16","children":["$","div",null,{"className":"flex gap-12","children":[["$","$L2",null,{"currentPath":"/docs/distributed"}],["$","main",null,{"className":"flex-1 min-w-0","children":[["$","$L3",null,{"currentPath":"/docs/distributed"}],["$","h1",null,{"className":"page-title mt-4","children":"Distributed Execution"}],["$","div",null,{"className":"prose prose-slate max-w-none","children":[["$","p",null,{"className":"text-lg text-muted mb-8","children":"MIND provides first-class support for distributed training and inference, allowing you to scale models across multiple nodes with automatic sharding, gradient synchronization, and fault tolerance."}],["$","div",null,{"className":"bg-indigo-50 border border-indigo-200 rounded-xl p-6 mb-8","children":["$","p",null,{"className":"text-sm text-indigo-800 m-0","children":[["$","strong",null,{"children":"Early Access:"}]," Distributed execution is currently in Phase 14 development. The APIs described here are subject to change. See the ",["$","$L4",null,{"href":"/roadmap","className":"text-indigo-600 underline","children":"Roadmap"}]," for current status."]}]}],["$","h2",null,{"className":"text-2xl font-bold font-heading mt-12 mb-4","children":"Overview"}],["$","p",null,{"className":"text-muted mb-4","children":"MIND's distributed execution framework supports three parallelism strategies:"}],["$","ul",null,{"className":"list-disc pl-6 space-y-2 text-muted mb-8","children":[["$","li",null,{"children":[["$","strong",null,{"children":"Data Parallelism"}],": Replicate the model across devices and split data batches"]}],["$","li",null,{"children":[["$","strong",null,{"children":"Model Parallelism"}],": Split large models across devices when they don't fit in memory"]}],["$","li",null,{"children":[["$","strong",null,{"children":"Pipeline Parallelism"}],": Split model layers across devices for improved throughput"]}]]}],["$","h2",null,{"className":"text-2xl font-bold font-heading mt-12 mb-4","children":"Getting Started"}],["$","p",null,{"className":"text-muted mb-4","children":["Enable distributed execution with the ",["$","code",null,{"className":"bg-slate-100 px-1.5 py-0.5 rounded text-sm","children":"@distributed"}]," annotation:"]}],["$","$L5",null,{"className":"mb-8","children":"use mind::distributed::{init, world_size, rank};\n\n@distributed(strategy = \"data_parallel\")\nfn train_step(model: &mut Model, batch: Tensor<f32, [B, 784]>) -> f32 {\n    let output = model.forward(batch);\n    let loss = cross_entropy(output, labels);\n\n    // Gradients are automatically synchronized across all nodes\n    loss.backward();\n    optimizer.step();\n\n    loss.item()\n}\n\nfn main() {\n    // Initialize distributed runtime\n    init();\n\n    println!(\"Running on node {} of {}\", rank(), world_size());\n\n    for epoch in 0..100 {\n        let loss = train_step(&mut model, batch);\n        if rank() == 0 {\n            println!(\"Epoch {}: loss = {:.4}\", epoch, loss);\n        }\n    }\n}"}],["$","h2",null,{"className":"text-2xl font-bold font-heading mt-12 mb-4","children":"Data Parallelism"}],["$","p",null,{"className":"text-muted mb-4","children":"Data parallelism replicates the model on each device and splits input batches. Gradients are averaged across all replicas using all-reduce operations."}],["$","$L5",null,{"className":"mb-8","children":"use mind::distributed::{DataParallel, AllReduce};\n\n// Wrap model for data parallel training\nlet model = DataParallel::new(model, devices);\n\n// Training loop - batches are automatically distributed\nfor batch in dataloader.iter() {\n    let loss = model.forward(batch);\n    loss.backward();\n\n    // Gradients synchronized via NCCL/Gloo\n    optimizer.step();\n}"}],["$","h3",null,{"className":"text-xl font-bold font-heading mt-8 mb-3","children":"Gradient Synchronization"}],"$L6","$L7","$L8","$L9","$La","$Lb","$Lc","$Ld","$Le","$Lf","$L10","$L11","$L12","$L13","$L14","$L15","$L16","$L17"]}],"$L18"]}]]}]}],["$L19"],"$L1a"]}],"loading":null,"isPartial":false}
6:["$","p",null,{"className":"text-muted mb-4","children":"MIND supports multiple collective communication backends:"}]
7:["$","div",null,{"className":"overflow-x-auto mb-8","children":["$","table",null,{"className":"min-w-full text-sm","children":[["$","thead",null,{"children":["$","tr",null,{"className":"border-b","children":[["$","th",null,{"className":"text-left py-2 pr-4 font-bold","children":"Backend"}],["$","th",null,{"className":"text-left py-2 pr-4 font-bold","children":"Devices"}],["$","th",null,{"className":"text-left py-2 font-bold","children":"Notes"}]]}]}],["$","tbody",null,{"className":"text-muted","children":[["$","tr",null,{"className":"border-b","children":[["$","td",null,{"className":"py-2 pr-4","children":"NCCL"}],["$","td",null,{"className":"py-2 pr-4","children":"NVIDIA GPU"}],["$","td",null,{"className":"py-2","children":"Recommended for multi-GPU training"}]]}],["$","tr",null,{"className":"border-b","children":[["$","td",null,{"className":"py-2 pr-4","children":"Gloo"}],["$","td",null,{"className":"py-2 pr-4","children":"CPU, GPU"}],["$","td",null,{"className":"py-2","children":"Cross-platform, supports TCP/IP"}]]}],["$","tr",null,{"className":"border-b","children":[["$","td",null,{"className":"py-2 pr-4","children":"MPI"}],["$","td",null,{"className":"py-2 pr-4","children":"CPU, GPU"}],["$","td",null,{"className":"py-2","children":"HPC clusters with InfiniBand"}]]}]]}]]}]}]
8:["$","h2",null,{"className":"text-2xl font-bold font-heading mt-12 mb-4","children":"Model Parallelism"}]
9:["$","p",null,{"className":"text-muted mb-4","children":"For models that don't fit on a single device, use model parallelism to split layers across devices:"}]
a:["$","$L5",null,{"className":"mb-8","children":"use mind::distributed::{ModelParallel, DeviceMap};\n\n// Define how layers map to devices\nlet device_map = DeviceMap::new()\n    .layer(\"embed\", Device::cuda(0))\n    .layers(\"transformer.0..12\", Device::cuda(0))\n    .layers(\"transformer.12..24\", Device::cuda(1))\n    .layer(\"head\", Device::cuda(1));\n\n// Create model parallel wrapper\nlet model = ModelParallel::new(model, device_map);\n\n// Forward pass automatically moves tensors between devices\nlet output = model.forward(input);"}]
b:["$","h2",null,{"className":"text-2xl font-bold font-heading mt-12 mb-4","children":"Pipeline Parallelism"}]
c:["$","p",null,{"className":"text-muted mb-4","children":"Pipeline parallelism enables higher throughput by overlapping computation across micro-batches:"}]
d:["$","$L5",null,{"className":"mb-8","children":"use mind::distributed::{Pipeline, Schedule};\n\n// Configure pipeline with micro-batching\nlet pipeline = Pipeline::new(model)\n    .stages(4)                    // Split into 4 pipeline stages\n    .micro_batches(8)             // 8 micro-batches per batch\n    .schedule(Schedule::GPipe);   // Or Schedule::PipeDream\n\n// Training with pipeline parallelism\nfor batch in dataloader.iter() {\n    let loss = pipeline.forward_backward(batch);\n    optimizer.step();\n}"}]
e:["$","h2",null,{"className":"text-2xl font-bold font-heading mt-12 mb-4","children":"Multi-Node Training"}]
f:["$","p",null,{"className":"text-muted mb-4","children":"Scale training across multiple machines using the MIND distributed launcher:"}]
10:["$","$L5",null,{"className":"mb-8","children":"# Launch on 4 nodes with 8 GPUs each\nmind launch --nodes 4 --gpus-per-node 8 \\\n    --master-addr 192.168.1.1 \\\n    --master-port 29500 \\\n    train.mind\n\n# Or use a hostfile\nmind launch --hostfile hosts.txt --gpus-per-node 8 train.mind"}]
11:["$","h2",null,{"className":"text-2xl font-bold font-heading mt-12 mb-4","children":"Fault Tolerance"}]
12:["$","p",null,{"className":"text-muted mb-4","children":"MIND supports elastic training with automatic checkpointing and recovery:"}]
13:["$","$L5",null,{"className":"mb-8","children":"use mind::distributed::{Elastic, Checkpoint};\n\n// Enable elastic training with checkpointing\nlet trainer = Elastic::new(model)\n    .min_nodes(2)\n    .max_nodes(8)\n    .checkpoint_dir(\"checkpoints/\")\n    .checkpoint_interval(1000);  // steps\n\n// Training automatically resumes on node failure\ntrainer.fit(dataloader, epochs);"}]
14:["$","h2",null,{"className":"text-2xl font-bold font-heading mt-12 mb-4","children":"Best Practices"}]
15:["$","ul",null,{"className":"list-disc pl-6 space-y-2 text-muted mb-8","children":[["$","li",null,{"children":"Start with data parallelism for most workloads - it's the simplest and most efficient"}],["$","li",null,{"children":"Use gradient accumulation to simulate larger batch sizes without more memory"}],["$","li",null,{"children":["Profile communication overhead with ",["$","code",null,{"className":"bg-slate-100 px-1.5 py-0.5 rounded text-sm","children":"MIND_PROFILE=1"}]]}],["$","li",null,{"children":"Enable mixed precision training to reduce communication bandwidth"}],["$","li",null,{"children":"Use gradient compression for slow network connections"}]]}]
16:["$","h2",null,{"className":"text-2xl font-bold font-heading mt-12 mb-4","children":"Learn More"}]
17:["$","p",null,{"className":"text-muted","children":["See the ",["$","$L4",null,{"href":"/docs/future","className":"text-primary hover:underline","children":"Future Extensions"}]," page for upcoming distributed features and the ",["$","$L4",null,{"href":"/roadmap","className":"text-primary hover:underline","children":"Roadmap"}]," for development status."]}]
18:["$","nav",null,{"aria-label":"Page navigation","className":"flex items-center justify-between mt-12 pt-8 border-t border-card-border","children":[["$","$L4",null,{"href":"/docs/ffi","className":"group flex items-center gap-2 text-muted hover:text-primary transition-colors","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":20,"height":20,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-left group-hover:-translate-x-1 transition-transform","aria-hidden":"true","children":[["$","path","1wnfg3",{"d":"m15 18-6-6 6-6"}],"$undefined"]}],["$","div",null,{"className":"text-left","children":[["$","span",null,{"className":"text-xs uppercase tracking-wider text-muted/60 block","children":"Previous"}],["$","span",null,{"className":"font-medium","children":"FFI & Bindings"}]]}]]}],["$","$L4",null,{"href":"/docs/deployment","className":"group flex items-center gap-2 text-muted hover:text-primary transition-colors text-right","children":[["$","div",null,{"children":[["$","span",null,{"className":"text-xs uppercase tracking-wider text-muted/60 block","children":"Next"}],["$","span",null,{"className":"font-medium","children":"Deployment"}]]}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":20,"height":20,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-right group-hover:translate-x-1 transition-transform","aria-hidden":"true","children":[["$","path","mthhwq",{"d":"m9 18 6-6-6-6"}],"$undefined"]}]]}]]}]
19:["$","script","script-0",{"src":"/_next/static/chunks/27be2e6eef9e968f.js","async":true}]
1a:["$","$L1b",null,{"children":["$","$1c",null,{"name":"Next.MetadataOutlet","children":"$@1d"}]}]
1d:null
